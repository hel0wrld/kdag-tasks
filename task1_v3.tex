\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{ragged2e}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cancel}

\title{KDAG tasks}
\author{Anmol Kumar}

\begin{document}

\maketitle

\section{First subtask}
\textbf{Problem:} Find the Hessian matrix H of the empirical loss function with respect to $\theta$, and show that the Hessian H is positive semi-definite in nature.

\begin{flushleft}
\textbf{Solution:} The empirical loss function is given as,
\begin{equation}
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{(i)} \log(h_\theta(x^{(i)}))+(1-y^{(i)})\log(1-h_\theta(x^{(i)}))
\end{equation}
Now defining $J(\theta)\cdot m$ as $L(\theta)$ and writing the loss expression for a single entry $i$, we get,
\begin{equation}
    L_{(i)}(\theta)=-[y^{(i)} \log(h_\theta(x^{(i)}))+(1-y^{(i)})\log(1-h_\theta(x^{(i)}))]
\end{equation}
The Hessian matrix H is composed of double derivatives of each of the elements, thus we find the double derivatives of the general element $L_{(i)}(\theta)$ we just calculated above. Note that we first differentiate w.r.t $\theta^T$ and then $\theta$ to get our desired value $\nabla_{\theta\theta^T}L_{(i)}(\theta)$\\
Now before we proceed further, we need to keep in mind an important relation, i.e,
\begin{equation}
    \frac{\partial\sigma(z)}{\partial z}=\sigma(z)(1-\sigma(z))
\end{equation}
where $\sigma(z)$ is defined as,
\begin{equation}
    \sigma(z)=\frac{1}{1+e^{-z}}
\end{equation}
In eq(2), our points of interest are $log(h_\theta(x^{(i)}))$ and $log(1-h_\theta(x^{(i)}))$, because the rest of eq(2) are constants. Labeling them as $L$ and $M$ respectively, we first find the gradient of these two functions.
\begin{equation}
\begin{aligned}
\frac{\partial L}{\partial \theta^T}
    &=\frac{\partial \log h_\theta(x^{(i)})}{\partial \theta^T}\\
    &=\frac{\partial \log \sigma(\theta^T x^{(i)})}{\partial \theta^T}\\
    &=\frac{\partial \log \sigma(\theta^T x^{(i)})}{\partial \sigma(\theta^T x^{(i)})}\cdot\frac{\partial \sigma(\theta^T x^{(i)})}{\partial \theta^T x^{(i)})}\cdot\frac{\partial \theta^T x^{(i)})}{\partial \theta^T}\\
    &=\frac{1}{\cancel{\sigma(\theta^T x^{(i)})}}\cdot\cancel{\sigma(\theta^T x^{(i)})}(1-\sigma(\theta^T x^{(i)})\cdot x^{(i)}\\
    &=(1-\sigma(\theta^T x^{(i)})x^{(i)}
\end{aligned}
\end{equation}
Similarly for M, we get,
\begin{equation}
\begin{aligned}
\frac{\partial M}{\partial \theta^T}
    &=\frac{\partial \log(1- h_\theta(x^{(i)}))}{\partial \theta^T}\\
    &=\frac{\partial \log(1- \sigma(\theta^T x^{(i)}))}{\partial \theta^T}\\
    &=\frac{\partial \log (1-\sigma(\theta^T x^{(i)}))}{\partial \sigma(\theta^T x^{(i)})}\cdot\frac{\partial \sigma(\theta^T x^{(i)})}{\partial \theta^T x^{(i)})}\cdot\frac{\partial \theta^T x^{(i)})}{\partial \theta^T}\\
    &=\frac{-1}{\cancel{1-\sigma(\theta^T x^{(i)})}}\cdot\sigma(\theta^T x^{(i)})(\cancel{1-\sigma(\theta^T x^{(i)})})\cdot x^{(i)}\\
    &=-\sigma(\theta^T x^{(i)}) x^{(i)}
\end{aligned}
\end{equation}
Putting all the values obtained, we get
\begin{equation}
    \nabla_{\theta^T} L_{(i)}(\theta)=x^{(i)}(\sigma(\theta^T x^{(i)})-y^{(i)})
\end{equation}
Evaluating further, 
\begin{equation}
\begin{aligned}
\nabla_{\theta\theta^T}L_{(i)}(\theta)&=\frac{{\partial^2} L_{(i)}(\theta)}{\partial \theta \partial \theta^T}\\
    &=\frac{\partial \nabla_{\theta^T} L_{(i)}(\theta)}{\partial \theta}\\
    &=\frac{\partial x^{(i)}(\sigma(\theta^T x^{(i)})-y^{(i)})}{\partial \theta}\\
    &=x^{(i)}[x^{(i)}]^T\sigma(\theta^T x^{(i)})(1-\sigma(\theta^T x^{(i)}))
\end{aligned}
\end{equation}
Thus, the Hessian matrix for $L_{(i)}(\theta)$ is given by the above expression. We can now find the Hessian matrix for our original empirical loss function $J_{(i)}(\theta)$
\begin{equation}
\begin{aligned}
L_{(i)}(\theta)&=m\cdot J_{(i)}(\theta)\\
\Rightarrow {\nabla}^2L_{(i)}(\theta)&=m\cdot{\nabla}^2 J_{(i)}(\theta)\\
\Rightarrow {\nabla}^2J_{(i)}(\theta)&=
\frac{1}{m}\cdot x^{(i)}[x^{(i)}]^T\sigma(\theta^T x^{(i)})(1-\sigma(\theta^T x^{(i)}))
\end{aligned}
\end{equation}
Note that the quantity $\sigma(\theta^T x^{(i)})(1-\sigma(\theta^T x^{(i)}))$ is $always >0$ as $\sigma(z)\in(0,1)$.\\
\vspace{6mm}
Considering each entry to be composed of $n$ features, we take $X$ as a matrix of dimensions $n\times m$, where every column represents $x^{(i)}$, which is the vector corresponding to a given entry, and every row represents a feature of that entry. Formally, $\sum_{i=1}^{m}x^{(i)}[x^{(i)}]^T=XX^T$ The number of columns is $m$, as its the number of entries for the particular data-set. For the factor of probability, we define a diagonal matrix $D$ of size $m\times m$, with $D_{ii}$ as $\frac{1}{m}\sigma(\theta^T x^{(i)})(1-\sigma(\theta^T x^{(i)}))$ for each set of inputs.\\
\vspace{6mm}
Therefore, using $X$ and $D$, we finally define our Hessian  $H$ as,
\begin{equation}
    H(\theta)=XDX^T
\end{equation}
To prove that $H$ is a positive semi-definite matrix, we need to show that the quantity $z^THz$, a scalar, is positive, where $z$ is any arbitrary matrix of dimensions $1\times n$, where $n$ is the number of features.
\begin{equation}
\begin{aligned}
z^THz&=z^TXDX^Tz&=(z^TX)D(z^TX)^T
\end{aligned}
\end{equation}
Since D is a positive contributing entity and $z^TX$ is being multiplied with itself, the whole scalar turns out to be non-negative.\\
\vspace{10mm}
Hence, the Hessian matrix $H$ has been proved to be positive semi-definite in nature.
\end{flushleft}
\section{Third subtask}
\textbf{Problem:} In order to show that Gaussian Discriminant Analysis results in a classifier that has a linear decision boundary, show that the following expression is true.
\begin{equation}
    p(y=1|x;\phi,\mu_0,\mu_1,\Sigma)=\frac{1}{1+\exp(-(\theta^T x+\theta_0))}
\end{equation}
where $\theta \in \Re^n$ and $\theta_0 \in \Re$ are appropriate functions of $\phi, \mu_0, \mu_1$ and $\Sigma$.
\begin{flushleft}
\textbf{Solution:} To show that the classifier has a linear decision boundary, we need to show that it has behavior like that of a logistic regression, as a LR classifies data on the basis of a dividing line.\\
\vspace{6mm}
Before we begin, we list the formulas that we have be given,
\begin{equation}
\begin{aligned}
p(y)=
      \begin{cases}
      \phi & \text{if $y=1$}\\
      1-\phi & \text{if $y=0$}\\
      \end{cases}
\end{aligned}
\end{equation}
\begin{equation}
p(x|y=0)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}\exp{(-\frac{1}{2}{(x-\mu_0)}^T \Sigma^{-1}(x-\mu_0))}
\end{equation}
\begin{equation}
p(x|y=1)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}\exp{(-\frac{1}{2}{(x-\mu_1)}^T \Sigma^{-1}(x-\mu_1))}
\end{equation}
To keep calculations simple, we denote $p(y=1|x;\phi,\mu_0,\mu_1,\Sigma)$ as $p(y=1|x).$ From Bayes theorem we know that,
\begin{equation}
p(y|x)&=\frac{p(x|y)p(y)}{p(x)}
\end{equation}
where, $ p(x)=p(x|y=1)p(y=1)+p(x|y=0)p(y=0)$.\\
\vspace{2mm}
Putting the value of $p(x)$ and using eq(16) for finding $p(y=1|x)$, we get,\\
\begin{equation}
\begin{aligned}
p(y=1|x)&=\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1)+p(x|y=0)p(y=0)}\\
       &=\frac{1}{1+\frac{p(x|y=0)}{p(x|y=1)}\frac{p(y=0)}{p(y=1)}}
\end{aligned}
\end{equation}
Using the values of $p(x|y=0)$, $p(x|y=1)$ and $p(y)$ from eq(13) to eq(15) we get,
\begin{equation}
    =\frac{1}{1+\exp{(Z)}(\frac{1-\phi}{\phi})}=\frac{1}{1+\exp{(Z')}}
\end{equation}
where, 
\begin{equation}
\begin{aligned}
    Z'&=-\frac{1}{2}(x-\mu_0)^T \Sigma^{-1}(x-\mu_0)+\frac{1}{2}(x-\mu_1)^T \Sigma^{-1}(x-\mu_1)+\log(\frac{1-\phi}{\phi})\\
    &=-\frac{1}{2}(x^T-\mu_0^T) \Sigma^{-1}(x-\mu_0)+\frac{1}{2}(x^T-\mu_1^T) \Sigma^{-1}(x-\mu_1)+\log(\frac{1-\phi}{\phi})\\
\end{aligned}
\end{equation}\\
\vspace{6mm}
As $\mu_0, \mu_1, x$ have the same dimensions i.e $n \times 1$ and the covariance matrix $\Sigma$ has dimensions $n\times n$, $(\mu_0-\mu_1)^T\Sigma^{-1}x=x^T\Sigma^{-1}(\mu_0-\mu_1)$. Using this expression and simplifying, we finally get,
\begin{equation}
\begin{aligned}
Z'&=-(\mu_1-\mu_0)^T\Sigma^{-1}x+[\log(\frac{1-\phi}{\phi})+\frac{1}{2}(\mu_1^T\Sigma^{-1}\mu_1-\mu_0^T\Sigma^{-1}\mu_0)]\\
&=-(\theta^Tx+\theta_0)
\end{aligned}
\end{equation}
Note that the terms in the square bracket simplifies to a scalar $-\theta_0$ and $\theta^T=(\mu_1-\mu_0)^T\Sigma^{-1}x$. Putting this in eq(18), we finally get our desired expression,
\begin{equation}
    p(y=1|x;\phi,\mu_0,\mu_1,\Sigma)=\frac{1}{1+\exp{(-(\theta^Tx+\theta_0))}}
\end{equation}\\
\vspace{8mm}
\end{flushleft}
As discussed earlier, this proves that GDA takes the form of logistic regression and hence, has a linear decision boundary.
\end{document}
